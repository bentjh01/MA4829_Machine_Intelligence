{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Analysis\n",
    "\n",
    "- attempt to predict for customisation likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(df):\n",
    "    res_dict = {}\n",
    "    for col in df.columns:\n",
    "      try:\n",
    "        res_dict[col+'_classes'] = [df[str(col)].unique()]\n",
    "      except:\n",
    "         print(f'Error with {col}')\n",
    "         pass\n",
    "    res_df = pd.DataFrame(data=res_dict)\n",
    "    display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'survey_results.csv'\n",
    "raw_df= pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classes(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fusing and Cleansing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = target_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_percentage(df, col, show = False):\n",
    "  counts = df[col].value_counts()\n",
    "  percentages = (counts / counts.sum()) * 100\n",
    "  for index, value in percentages.items():\n",
    "    if value < 5 or show:\n",
    "        print('_'*80)\n",
    "        print(col)\n",
    "        print('_'*80)\n",
    "        print(percentages)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'Which age group do you belong to?': 'age_group', \n",
    "                'What is your gender?': 'gender', \n",
    "                \"Which category do you currently belong to?\": \"car_ownership\",\n",
    "                \"Which of the following best describes you?\": \"maritial_status\",\n",
    "                \"Which of these factors are important to you when deciding which car to purchase?\": \"purchase_factors\",\n",
    "                \"How likely are you to opt for customised vehicle if there were no extra charges? \": \"customise_likelihood\",\n",
    "                \"Which of the following exterior components would you choose to customise (texture, layout, size, etc)? \": \"exterior_components\",\n",
    "                \"Which of the following interior components would you choose to customise (texture, layout, size, etc)? \": \"interior_components\",\n",
    "                \"How much are you willing to spend on car customisation if surcharges are applicable?\": \"customise_spend\",\n",
    "                \"Are you interested in designing your own components to personalise your car? \": \"personalise_interest\",\n",
    "                \"How much are you willing to pay for the personalised design? \": \"personalise_spend\",\n",
    "                \"Do you have any 3D design experience that would help with the design process? (e.g. AutoCAD, SolidWorks, Blender, etc)\": \"design_experience\",\n",
    "                \"Please give us any design ideas to make the car uniquely Singaporean.\": \"design_ideas\"\n",
    "                }\n",
    "\n",
    "preprocessed_df.rename(columns=rename_dict, inplace=True)\n",
    "df = pd.DataFrame(rename_dict.items(), columns=[\"Initial\", \"Renamed\"]) \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocessed_df.dropna(subset=['exterior_components', 'purchase_factors'])\n",
    "preprocessed_df = preprocessed_df[preprocessed_df['gender'] != 'Prefer not to say']\n",
    "preprocessed_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lists in Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['purchase_factors'] = preprocessed_df['purchase_factors'].str.split(';')\n",
    "preprocessed_df['exterior_components'] = preprocessed_df['exterior_components'].str.split(';')\n",
    "preprocessed_df['interior_components'] = preprocessed_df['interior_components'].str.split(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging similar data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in preprocessed_df.columns:\n",
    "#   check_percentage(preprocessed_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_df = preprocessed_df[preprocessed_df['customise_likelihood'] != 'Not likely']\n",
    "# preprocessed_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['customise_spend'] = preprocessed_df.apply(lambda row: 'under 500' if row['customise_spend'] == '100-500' else row['customise_spend'], axis=1)\n",
    "# preprocessed_df['customise_spend'] = preprocessed_df.apply(lambda row: 'under 500' if row['customise_spend'] == '0' else row['customise_spend'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_df['personalise_spend'] = preprocessed_df.apply(lambda row: 'under 500' if row['personalise_spend'] == '0' else row['personalise_spend'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tersing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['car_ownership'] = preprocessed_df.apply(lambda row: 'None' if row['car_ownership'] == 'Do not own a car, but planning to purchase in future' else row['car_ownership'], axis=1)\n",
    "preprocessed_df['car_ownership'] = preprocessed_df.apply(lambda row: 'One' if row['car_ownership'] == 'Own a car' else row['car_ownership'], axis=1)\n",
    "preprocessed_df['car_ownership'] = preprocessed_df.apply(lambda row: 'More than One' if row['car_ownership'] == 'Own more than one car' else row['car_ownership'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['design_experience'] = preprocessed_df.apply(lambda row: 1 if row['design_experience'] == 'Yes, I can design on my own' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filling na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['design_ideas'] = preprocessed_df['design_ideas'].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = preprocessed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_list_values(df, col):\n",
    "    res_df = df.copy()\n",
    "    dummy_df = pd.get_dummies(data=df[col].explode(), dtype=int, prefix=col, prefix_sep=':').groupby(level=0).sum()\n",
    "    res_df = pd.concat([df, dummy_df], axis=1)\n",
    "    # res_df = res_df.drop(columns=[col])\n",
    "    return res_df\n",
    "def expand_non_binary_values(df, col):\n",
    "    res_df = df.copy()\n",
    "    dummy_df = pd.get_dummies(data=df[col], dtype=int, prefix=col, prefix_sep=':').groupby(level=0).sum()\n",
    "    res_df = pd.concat([df, dummy_df], axis=1)\n",
    "    res_df = res_df.drop(columns=[col])\n",
    "    return res_df\n",
    "def encode_data_by_count(column_name, data_frame):\n",
    "  print(\"_\"*120)\n",
    "  label_encoder = LabelEncoder()\n",
    "  encoded_column_name = column_name+\"_encoded\"\n",
    "  data_frame[encoded_column_name] = label_encoder.fit_transform(data_frame[column_name])\n",
    "  legend = data_frame[[column_name, encoded_column_name]].copy()\n",
    "  legend = legend.drop_duplicates().reset_index(drop=True)\n",
    "  print(legend)\n",
    "def encode_by_label(df, unique_values, col):\n",
    "  print(\"_\"*120)\n",
    "  encoded_column_name = col+\"_encoded\"\n",
    "  for i, val in enumerate(unique_values):\n",
    "    if val not in df[col].unique():\n",
    "      print(f\"'{val}' not in {col}\")\n",
    "      return\n",
    "  label_mapping = {val: i for i, val in enumerate(unique_values)}\n",
    "  df[encoded_column_name] = df[col].map(label_mapping)\n",
    "  legend = df[[col, encoded_column_name]].copy()\n",
    "  legend = legend.drop_duplicates()\n",
    "  legend = legend.sort_values(by=encoded_column_name)\n",
    "  legend = legend.reset_index(drop=True)\n",
    "  print(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df=expand_list_values(transformed_df, 'purchase_factors')\n",
    "transformed_df=expand_list_values(transformed_df, 'exterior_components')\n",
    "transformed_df=expand_list_values(transformed_df, 'interior_components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['design_ideas'] = preprocessed_df.apply(lambda row: 0 if row['design_ideas'] == 'none' else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classes(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data_by_count('gender', transformed_df)\n",
    "encode_data_by_count('maritial_status', transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_by_label(transformed_df, ['0', 'under 500', '500-1000', 'over 1000'], 'customise_spend')\n",
    "encode_by_label(transformed_df, ['0', 'under 500', '500-1000', 'over 1000'], 'personalise_spend')\n",
    "encode_by_label(transformed_df, ['None', 'One', 'More than One'], 'car_ownership')\n",
    "encode_by_label(transformed_df, ['Not likely', 'Likely', 'Very likely'], 'customise_likelihood')\n",
    "encode_by_label(transformed_df, ['No', 'Only with professional help','Yes'], 'personalise_interest')\n",
    "encode_by_label(transformed_df, ['20-30', '31-40', '41-50', '51-60'], 'age_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_col_sel = [\n",
    "  'gender_encoded',\n",
    "  'maritial_status_encoded',\n",
    "  'car_ownership_encoded',\n",
    "  'age_group_encoded',\n",
    "  'purchase_factors:Aesthetics',\n",
    "  'purchase_factors:Brand name',\n",
    "  'purchase_factors:Customisable options',\n",
    "  'purchase_factors:Functionality',\n",
    "  'purchase_factors:Price',\n",
    "  'purchase_factors:Size',\n",
    "  'purchase_factors:Sustainability/environment considerations',\n",
    "  'purchase_factors:Technological features'\n",
    "]\n",
    "purchase_df = transformed_df[purchase_col_sel]\n",
    "\n",
    "purchase_factors = transformed_df['purchase_factors'].explode().unique().tolist()\n",
    "for i, val in enumerate(purchase_factors):\n",
    "  purchase_factors[i] = 'purchase_factors:' + val\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "purchase_df = pd.DataFrame(data = scaler.fit_transform(purchase_df), columns = purchase_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced\n",
    "reduced_purchase_df = purchase_df.copy()\n",
    "pca = PCA()\n",
    "pca.fit(reduced_purchase_df)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(np.arange(1, len(prop_var)+1), \n",
    "                   prop_var, marker='o')\n",
    "plt.xlabel('Principal Component',\n",
    "           size = 20)\n",
    "plt.ylabel('Proportion of Variance Explained',\n",
    "           size = 20)\n",
    "plt.title('Figure 1: Scree Plot for Proportion of Variance Explained for Purchase Data',\n",
    "          size = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "reduced_purchase_df = PCA(n_components=10).fit_transform(reduced_purchase_df)\n",
    "reduced_purchase_df = pd.DataFrame(reduced_purchase_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customise DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Selection\n",
    "customise_col_sel = [\n",
    "  'exterior_components:Brakes',\n",
    "  'exterior_components:Bumpers',\n",
    "  'exterior_components:Grilles',\n",
    "  'exterior_components:Headlights',\n",
    "  'exterior_components:Side mirrors',\n",
    "  'exterior_components:Wheels',\n",
    "  'exterior_components:add body kit and change the exhaust and tune the engine',\n",
    "  'exterior_components:doors',\n",
    "  'interior_components:Centre compartment',\n",
    "  'interior_components:Dashboard',\n",
    "  'interior_components:Door handles',\n",
    "  'interior_components:Steering wheel',\n",
    "  'interior_components:Sun blocker for front passengers',\n",
    "  'interior_components:air vent', \n",
    "  'customise_spend_encoded',\n",
    "  'personalise_spend_encoded'\n",
    "]\n",
    "customise_df = transformed_df[customise_col_sel]\n",
    "\n",
    "interior_components = transformed_df['interior_components'].explode().unique().tolist()\n",
    "for i, val in enumerate(interior_components):\n",
    "  interior_components[i] = 'interior_components:' + val\n",
    "exterior_components = transformed_df['exterior_components'].explode().unique().tolist()\n",
    "for i, val in enumerate(exterior_components):\n",
    "  exterior_components[i] = 'exterior_components:' + val\n",
    "\n",
    "# Scaling Min MAx\n",
    "scaler = MinMaxScaler()\n",
    "customise_df = pd.DataFrame(data = scaler.fit_transform(customise_df), columns = customise_df.columns)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced\n",
    "reduced_customise_df = customise_df.copy()\n",
    "reduced_customise_df.drop(columns=['customise_spend_encoded', 'personalise_spend_encoded'], inplace=True)\n",
    "pca = PCA()\n",
    "pca.fit(reduced_customise_df)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(np.arange(1, len(prop_var)+1), \n",
    "                   prop_var, marker='o')\n",
    "plt.xlabel('Principal Component',\n",
    "           size = 20)\n",
    "plt.ylabel('Proportion of Variance Explained',\n",
    "           size = 20)\n",
    "plt.title('Figure 1: Scree Plot for Proportion of Variance Explained for Customised Components Data',\n",
    "          size = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "reduced_customise_df = PCA(n_components=10).fit_transform(reduced_customise_df)\n",
    "reduced_customise_df = pd.concat([customise_df.loc[:,['customise_spend_encoded', 'personalise_spend_encoded']], pd.DataFrame(reduced_customise_df)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns = [col if 'encoded'in col else None for col in transformed_df.columns]\n",
    "select_columns = [i for i in select_columns if i != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = transformed_df.copy()\n",
    "select_columns = [col if 'encoded'in col else None for col in transformed_df.columns]\n",
    "select_columns += interior_components+exterior_components\n",
    "\n",
    "for col in reduced_df.columns:\n",
    "  if col not in select_columns:\n",
    "    reduced_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df = reduced_df.copy()\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(data = scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df.drop(['customise_spend_encoded', 'personalise_spend_encoded', 'personalise_interest_encoded', 'customise_likelihood_encoded'], axis=1))\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(np.arange(1, len(prop_var)+1), \n",
    "                   prop_var, marker='o')\n",
    "plt.xlabel('Principal Component',\n",
    "           size = 20)\n",
    "plt.ylabel('Proportion of Variance Explained',\n",
    "           size = 20)\n",
    "plt.title('Figure 1: Scree Plot for Proportion of Variance Explained for Customer',\n",
    "          size = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "reduced_df = PCA(n_components=12).fit_transform(reduced_df.drop('customise_spend_encoded', axis=1))\n",
    "reduced_df = pd.concat([transformed_df['customise_spend_encoded'], pd.DataFrame(reduced_df)], axis=1)\n",
    "\n",
    "display(reduced_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_components_df = transformed_df.copy()\n",
    "\n",
    "select_columns = ['customise_spend_encoded']+interior_components+exterior_components\n",
    "for col in reduced_components_df.columns:\n",
    "  if col not in select_columns:\n",
    "    reduced_components_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df = reduced_components_df.copy()\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(data = scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df.drop('customise_spend_encoded', axis=1))\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(np.arange(1, len(prop_var)+1), \n",
    "                   prop_var, marker='o')\n",
    "plt.xlabel('Principal Component',\n",
    "           size = 20)\n",
    "plt.ylabel('Proportion of Variance Explained',\n",
    "           size = 20)\n",
    "plt.title('Figure 1: Scree Plot for Proportion of Variance Explained for customised components',\n",
    "          size = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "reduced_components_df = PCA(n_components=10).fit_transform(reduced_components_df.drop('customise_spend_encoded', axis=1))\n",
    "reduced_components_df = pd.concat([transformed_df['customise_spend_encoded'], pd.DataFrame(reduced_components_df)], axis=1)\n",
    "\n",
    "display(reduced_components_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=transformed_df.copy()\n",
    "reduced_data = reduced_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_curve,roc_auc_score,confusion_matrix,classification_report\n",
    "from arulespy.arules import Transactions, apriori, parameters, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, X_train, X_test, y_train, y_test, print_results=True):\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  if print_results:\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"Precision Score: \\n{precision_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"Recall Score: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Classification Report: \\n{classification_report(y_test, y_pred)}\")\n",
    "  # tp, fn, fp, tn = confusion_matrix(y_test,y_pred).reshape(-1)\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "  precision = precision_score(y_test, y_pred, average='weighted')\n",
    "  recall = recall_score(y_test, y_pred, average='weighted')\n",
    "  model_dict={'Model': [model.__class__.__name__], 'Accuracy': [round(accuracy,3)], 'F1_Score': [round(f1,3)], 'Precision': [round(precision,3)], 'Recall': [round(recall,3)]}\n",
    "  try:\n",
    "    model_df = pd.DataFrame(data=model_dict)\n",
    "  except:\n",
    "    print(\"Error creating model_df\")\n",
    "    print(model_dict)\n",
    "    return\n",
    "  return model, model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), ExtraTreesClassifier(), KNeighborsClassifier(), GaussianNB()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "plt.axis('off')\n",
    "select_columns = ['age_group', 'gender',\n",
    "                  'car_ownership', 'maritial_status', \n",
    "                  'customise_likelihood', 'customise_spend', \n",
    "                  'personalise_interest', 'personalise_spend',\n",
    "                  'design_experience', 'design_ideas']\n",
    "for i, col in enumerate(select_columns):\n",
    "  plt.subplot(5, 2, i+1)\n",
    "  sns.countplot(data=data, y=col, orient='h')#, hue='customise_spend_encoded')\n",
    "  plt.title(col)\n",
    "  plt.xlim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "select_columns = ['purchase_factors', \n",
    "                  'interior_components', \n",
    "                  'exterior_components']\n",
    "for i, col in enumerate(select_columns):\n",
    "  plt.subplot(3, 1, i+1)\n",
    "  sns.barplot(data=data[col].explode().value_counts(),orient='h')\n",
    "  plt.title(col)\n",
    "  plt.xlim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.copy()\n",
    "dropped_columns = [ 'age_group', 'gender',\n",
    "                    'car_ownership', 'maritial_status', \n",
    "                    'customise_likelihood', 'customise_spend', \n",
    "                    'personalise_interest', 'personalise_spend',\n",
    "                    'design_experience', 'design_ideas', \n",
    "                    'purchase_factors', 'interior_components', \n",
    "                    'exterior_components'\n",
    "                    ]\n",
    "test_data.drop(dropped_columns, axis=1, inplace=True)\n",
    "corr = test_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "customise_features = []\n",
    "for val in corr['customise_spend_encoded']:\n",
    "  factor = 0.2\n",
    "  if val > factor or val < -factor:\n",
    "    features_dict[corr['customise_spend_encoded'][corr['customise_spend_encoded'] == val].index[0]] = [val]\n",
    "    customise_features.append(corr['customise_spend_encoded'][corr['customise_spend_encoded'] == val].index[0])\n",
    "display(pd.DataFrame(data=features_dict).transpose().rename(columns={0:'pearson_correlation to customise_spend'}).sort_values(by='pearson_correlation to customise_spend', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "personalise_features = []\n",
    "for val in corr['personalise_spend_encoded']:\n",
    "  factor = 0.2\n",
    "  if val > factor or val < -factor:\n",
    "    features_dict[corr['personalise_spend_encoded'][corr['personalise_spend_encoded'] == val].index[0]] = [val]\n",
    "    personalise_features.append(corr['personalise_spend_encoded'][corr['personalise_spend_encoded'] == val].index[0])\n",
    "display(pd.DataFrame(data=features_dict).transpose().rename(columns={0:'pearson_correlation to personalise_spend'}).sort_values(by='pearson_correlation to personalise_spend', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the correlation matrix, we can conclude that there is a strong positive correlation between the amount one is willing to spend  \n",
    "on a customisation and:\n",
    "\n",
    "- exterior_components:Wheels\n",
    "- gender_encoded\n",
    "- interior_components:Steering wheel\n",
    "- purchase_factors:Sustainability/environment considerations\n",
    "\n",
    "</br>\n",
    "Therefore, you may increase these aspects to allow for more revenue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_data = data.copy()\n",
    "for col in modeled_data.columns:\n",
    "  if col not in customise_features:\n",
    "    if col == 'customise_spend_encoded':\n",
    "      continue\n",
    "    modeled_data.drop(col, axis=1, inplace=True)\n",
    "modeled_data.drop('personalise_spend_encoded', axis=1, inplace=True)\n",
    "# modeled_data.drop('customise_likelihood_encoded', axis=1, inplace=True)\n",
    "# modeled_data.drop('personalise_interest_encoded', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = modeled_data['customise_spend_encoded']\n",
    "X = modeled_data.drop(['customise_spend_encoded'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "res = pd.DataFrame(columns=['Model', 'Accuracy', 'F1_Score', 'Precision', 'Recall'])\n",
    "customised_vehicle_likelihood_model = {}\n",
    "for model in models:\n",
    "  model_res, model_df = model_evaluation(model, X_train, X_test, y_train, y_test, False)\n",
    "  res = pd.concat([res, model_df], ignore_index=True)\n",
    "  customised_vehicle_likelihood_model[model.__class__.__name__] = model_res\n",
    "  del model_res\n",
    "  del model_df\n",
    "res = res.sort_values(by='Accuracy', ascending=False)\n",
    "res = res.reset_index(drop=True)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = reduced_df.copy()\n",
    "df.drop('customise_spend_encoded', axis=1, inplace=True)\n",
    "df = pd.DataFrame(data = scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "reduced_df_X = df\n",
    "reduced_df_y = reduced_df['customise_spend_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_df_X, reduced_df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(reduced_df_X.to_numpy())\n",
    "\n",
    "u_labels = np.unique(clusters)\n",
    " \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in u_labels:\n",
    "    plt.scatter(reduced_df_X.to_numpy()[clusters == i , 0] , reduced_df_X.to_numpy()[clusters == i , 1] , label = i)\n",
    "plt.title('KMeans Clustering on Reduced Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['Model', 'Accuracy', 'F1_Score', 'Precision', 'Recall'])\n",
    "customised_vehicle_likelihood_model = {}\n",
    "for model in models:\n",
    "  model_res, model_df = model_evaluation(model, X_train, X_test, y_train, y_test, False)\n",
    "  res = pd.concat([res, model_df], ignore_index=True)\n",
    "  customised_vehicle_likelihood_model[model.__class__.__name__] = model_res\n",
    "  del model_res\n",
    "  del model_df\n",
    "res = res.sort_values(by='Accuracy', ascending=False)\n",
    "res = res.reset_index(drop=True)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on reduced_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = reduced_components_df.copy()\n",
    "df.drop('customise_spend_encoded', axis=1, inplace=True)\n",
    "df = pd.DataFrame(data = scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "reduced_components_df_X = df\n",
    "reduced_components_df_y = reduced_df['customise_spend_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_components_df_X, reduced_components_df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(reduced_components_df_X.to_numpy())\n",
    "\n",
    "u_labels = np.unique(clusters)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in u_labels:\n",
    "    plt.scatter(reduced_components_df_X.to_numpy()[clusters == i , 0] , reduced_components_df_X.to_numpy()[clusters == i , 1] , label = i)\n",
    "plt.title('KMeans Clustering on Reduced Customised Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on reduced_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['Model', 'Accuracy', 'F1_Score', 'Precision', 'Recall'])\n",
    "customised_vehicle_likelihood_model = {}\n",
    "for model in models:\n",
    "  model_res, model_df = model_evaluation(model, X_train, X_test, y_train, y_test, False)\n",
    "  res = pd.concat([res, model_df], ignore_index=True)\n",
    "  customised_vehicle_likelihood_model[model.__class__.__name__] = model_res\n",
    "  del model_res\n",
    "  del model_df\n",
    "res = res.sort_values(by='Accuracy', ascending=False)\n",
    "res = res.reset_index(drop=True)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules on Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_factors_df = data.loc[:, purchase_factors]\n",
    "for col in purchase_factors_df.columns:\n",
    "  purchase_factors_df[col] = purchase_factors_df.apply(lambda row: True if row[col] == 1 else False, axis=1)\n",
    "\n",
    "trans = Transactions.from_df(purchase_factors_df)\n",
    "\n",
    "rules = apriori(trans,\n",
    "                    parameter = parameters({\"supp\": 0.1, \"conf\": 0.8}), \n",
    "                    control = parameters({\"verbose\": False}))  \n",
    "rules_df = rules.as_df()\n",
    "rules_df = rules_df.sort_values(by='lift', ascending=False)\n",
    "rules_df.to_csv('purchase_factors_rules.csv')\n",
    "display(rules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exterior_components_df = data.loc[:, exterior_components]\n",
    "for col in exterior_components_df.columns:\n",
    "  exterior_components_df[col] = exterior_components_df.apply(lambda row: True if row[col] == 1 else False, axis=1)\n",
    "\n",
    "trans = Transactions.from_df(exterior_components_df)\n",
    "\n",
    "rules = apriori(trans,\n",
    "                    parameter = parameters({\"supp\": 0.1, \"conf\": 0.8}), \n",
    "                    control = parameters({\"verbose\": False}))  \n",
    "rules_df = rules.as_df()\n",
    "rules_df = rules_df.sort_values(by='lift', ascending=False)\n",
    "rules_df.to_csv('exterior_components_rules.csv')\n",
    "display(rules_df['RHS'].value_counts().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interior_components_df = data.loc[:, interior_components]\n",
    "for col in interior_components_df.columns:\n",
    "  interior_components_df[col] = interior_components_df.apply(lambda row: True if row[col] == 1 else False, axis=1)\n",
    "\n",
    "trans = Transactions.from_df(interior_components_df)\n",
    "\n",
    "rules = apriori(trans,\n",
    "                    parameter = parameters({\"supp\": 0.1, \"conf\": 0.8}), \n",
    "                    control = parameters({\"verbose\": False}))  \n",
    "rules_df = rules.as_df()\n",
    "rules_df = rules_df.sort_values(by='lift', ascending=False)\n",
    "rules_df.to_csv('interior_components_rules.csv')\n",
    "display(rules_df['RHS'].value_counts().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = data.loc[:, interior_components+exterior_components]\n",
    "for col in components_df.columns:\n",
    "  components_df[col] = components_df.apply(lambda row: True if row[col] == 1 else False, axis=1)\n",
    "\n",
    "trans = Transactions.from_df(components_df)\n",
    "\n",
    "rules = apriori(trans,\n",
    "                    parameter = parameters({\"supp\": 0.1, \"conf\": 0.8}), \n",
    "                    control = parameters({\"verbose\": False}))  \n",
    "rules_df = rules.as_df()\n",
    "rules_df = rules_df.sort_values(by='lift', ascending=False)\n",
    "rules_df.to_csv('components_df_rules.csv')\n",
    "display(rules_df['RHS'].value_counts().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arules_df = data.copy()\n",
    "select_columns = ['customise_spend_encoded']+interior_components+exterior_components\n",
    "for col in arules_df.columns:\n",
    "  if col not in select_columns:\n",
    "    arules_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "trans = Transactions.from_df(arules_df)\n",
    "\n",
    "rules = apriori(trans,\n",
    "                    parameter = parameters({\"supp\": 0.1, \"conf\": 0.8}), \n",
    "                    control = parameters({\"verbose\": False}))  \n",
    "rules_df = rules.sort(by = 'lift').as_df()\n",
    "customise_spend_df = rules_df.loc[rules_df['RHS'] == '{customise_spend_encoded=[2,3]}']\n",
    "customise_spend_df.to_csv('customise_spend_rules.csv')\n",
    "display(customise_spend_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
